{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 7705679,
          "sourceType": "datasetVersion",
          "datasetId": 4498747
        },
        {
          "sourceId": 11382,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 8318
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "RAG using Gemma, Langchain and ChromaDB",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahanave/found_it_using_gemma/blob/main/Found_it_using_Gemma%2C_Langchain_and_ChromaDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "memocan_data_science_interview_q_and_a_treasury_path = kagglehub.dataset_download('memocan/data-science-interview-q-and-a-treasury')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "EBaqFK8PSI-u"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>RAG using Gemma, Langchain and ChromaDB</h1></center>\n",
        "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This notebook demonstrates how to build a retrieval augmented generation (RAG) system using Gemma as a large language model (LLM), Langchain for tools to process input files, and ChromaDB as vector database.\n",
        "\n",
        "## What is RAG?\n",
        "\n",
        "Retriever augmented generation (RAG) is a system that improves the response generated by a LLM in two ways:\n",
        "- First, the information is retrieved from a dataset that is stored in vector database; the query is used to perform similarity search in the documents stored in the vector database.\n",
        "- Second, by restraining the context provided to the LLM to content that is similar with the initial query, stored in the vector database, we can reduce significantly (or even eliminate) LLM's halucinations, since the answer is provided from the context of the stored documents.\n",
        "\n",
        "An important advantage of this approach is that we do not need to fine-tune the LLM with our custom data; instead, the data is ingested (cleaned, transformed, chunked, and indexed in the vector database).\n",
        "\n",
        "## Procedure\n",
        "\n",
        "We create two classes:\n",
        "* AIAgent - An AI Agent that query Gemma LLM using a custom prompt that instruct Gemma to generate and answer (from the query) by refering to the context (as well provided); the answer to the AI Agent query function is then returned.\n",
        "* RAGSystem - initialized with the dataset with Data Science information, with an AIAgent object. In the init function of this class, we ingest the data from the dataset in the vector database. This class have as well a query member function. In this function we first perform similarity search with the query to the vector database. Then, we call the generate function of the ai agent object. Before returning the answer, we use a predefined template to compose the overal response from the question, answer and the context retrieved.\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "fRPL40GHSI-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages instalation and configurations"
      ],
      "metadata": {
        "id": "hnBjNZcGSI-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install required libraries\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install accelerate\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install langchain\n",
        "!pip install sentence-transformers\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T20:42:33.256721Z",
          "iopub.execute_input": "2024-04-05T20:42:33.257323Z",
          "iopub.status.idle": "2024-04-05T20:44:58.804489Z",
          "shell.execute_reply.started": "2024-04-05T20:42:33.257291Z",
          "shell.execute_reply": "2024-04-05T20:44:58.803391Z"
        },
        "trusted": true,
        "id": "fZOo6JZrSI-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from IPython.display import display, Markdown\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T20:44:58.806421Z",
          "iopub.execute_input": "2024-04-05T20:44:58.806721Z",
          "iopub.status.idle": "2024-04-05T20:45:04.553528Z",
          "shell.execute_reply.started": "2024-04-05T20:44:58.806697Z",
          "shell.execute_reply": "2024-04-05T20:45:04.55253Z"
        },
        "trusted": true,
        "id": "-jhgpVAESI-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Agent class"
      ],
      "metadata": {
        "id": "uO0GP-ReSI-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AIAgent:\n",
        "    \"\"\"\n",
        "    Gemma 2b-it assistant.\n",
        "    It uses Gemma transformers 2b-it/2.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_length=256):\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/2\")\n",
        "        self.gemma_lm = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/2\")\n",
        "\n",
        "    def create_prompt(self, query, context):\n",
        "        # prompt template\n",
        "        prompt = f\"\"\"\n",
        "        You are an AI Agent specialized to answer to questions about Data Science.\n",
        "        Explain the concept or answer the question about Data Science.\n",
        "        In order to create the answer, please only use the information from the\n",
        "        context provided (Context). Do not include other information.\n",
        "        Answer with simple words.\n",
        "        If needed, include also explanations.\n",
        "        Question: {query}\n",
        "        Context: {context}\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def generate(self, query, retrieved_info):\n",
        "        prompt = self.create_prompt(query, retrieved_info)\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "        # Answer generation\n",
        "        answer = self.gemma_lm.generate(\n",
        "            input_ids,\n",
        "            #max_length=self.max_length, # limit the answer to max_length\n",
        "            max_new_tokens=self.max_length\n",
        "        )\n",
        "        # Decode and return the answer\n",
        "        answer = self.tokenizer.decode(answer[0], skip_special_tokens=True, skip_prompt=True)\n",
        "        return prompt, answer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T21:50:55.681148Z",
          "iopub.execute_input": "2024-04-05T21:50:55.681769Z",
          "iopub.status.idle": "2024-04-05T21:50:55.689575Z",
          "shell.execute_reply.started": "2024-04-05T21:50:55.681737Z",
          "shell.execute_reply": "2024-04-05T21:50:55.68867Z"
        },
        "trusted": true,
        "id": "-Zxjp4OpSI-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the AIAgent"
      ],
      "metadata": {
        "id": "kRGAEodFSI-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_agent = AIAgent()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T21:51:00.576937Z",
          "iopub.execute_input": "2024-04-05T21:51:00.577802Z",
          "iopub.status.idle": "2024-04-05T21:51:06.024217Z",
          "shell.execute_reply.started": "2024-04-05T21:51:00.577769Z",
          "shell.execute_reply": "2024-04-05T21:51:06.023455Z"
        },
        "trusted": true,
        "id": "T5IXR6FDSI-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the context from the Data Science interview Q&A treasury."
      ],
      "metadata": {
        "id": "trWZTS9xSI-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 1000)\n",
        "data_df = pd.read_csv(\"/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv\")\n",
        "data_df.head(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T21:51:09.127058Z",
          "iopub.execute_input": "2024-04-05T21:51:09.127706Z",
          "iopub.status.idle": "2024-04-05T21:51:09.142835Z",
          "shell.execute_reply.started": "2024-04-05T21:51:09.127674Z",
          "shell.execute_reply": "2024-04-05T21:51:09.141983Z"
        },
        "trusted": true,
        "id": "hXuAgWxlSI-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = data_df.iloc[0].answer\n",
        "print(\"Context: \", context)\n",
        "prompt, answer = ai_agent.generate(query=\"What is supervised learning?\", retrieved_info=context)\n",
        "print(\"LLM Answer: \", answer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T21:51:13.377434Z",
          "iopub.execute_input": "2024-04-05T21:51:13.37813Z",
          "iopub.status.idle": "2024-04-05T21:51:50.460568Z",
          "shell.execute_reply.started": "2024-04-05T21:51:13.3781Z",
          "shell.execute_reply": "2024-04-05T21:51:50.459664Z"
        },
        "trusted": true,
        "id": "5-cyzUprSI-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Sentence embedding based Retrieval Based Augmented generation.\n",
        "        Given database of pdf files, retriever finds num_retrieved_docs relevant documents\"\"\"\n",
        "    def __init__(self, ai_agent, num_retrieved_docs=2):\n",
        "        # load the data\n",
        "        self.num_docs = num_retrieved_docs\n",
        "        self.ai_agent = ai_agent\n",
        "        loader = CSVLoader(\"/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv\")\n",
        "        documents = loader.load()\n",
        "        self.template = \"\\n\\nQuestion:\\n{question}\\n\\nPrompt:\\n{prompt}\\n\\nAnswer:\\n{answer}\\n\\nContext:\\n{context}\"\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=800,\n",
        "            chunk_overlap=100)\n",
        "        all_splits = text_splitter.split_documents(documents)\n",
        "        # create a vectorstore database\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "        self.vector_db = Chroma.from_documents(documents=all_splits,\n",
        "                                               embedding=embeddings,\n",
        "                                               persist_directory=\"chroma_db\")\n",
        "        self.retriever = self.vector_db.as_retriever()\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        # retrieve top k similar documents to query\n",
        "        docs = self.retriever.get_relevant_documents(query)\n",
        "        return docs\n",
        "\n",
        "    def query(self, query):\n",
        "        # generate the answer\n",
        "        context = self.retrieve(query)\n",
        "        data = \"\"\n",
        "        for item in list(context):\n",
        "            data += item.page_content\n",
        "\n",
        "        data = data[:500]\n",
        "\n",
        "        prompt, answer = self.ai_agent.generate(query, data)\n",
        "\n",
        "        return self.template.format(question=query,\n",
        "                                    prompt=prompt,\n",
        "                                   answer=answer,\n",
        "                                   context=context)\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T21:52:26.673255Z",
          "iopub.execute_input": "2024-04-05T21:52:26.674175Z",
          "iopub.status.idle": "2024-04-05T21:52:26.684815Z",
          "shell.execute_reply.started": "2024-04-05T21:52:26.674141Z",
          "shell.execute_reply": "2024-04-05T21:52:26.683831Z"
        },
        "trusted": true,
        "id": "1_j7dg8aSI-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def colorize_text(text):\n",
        "    for word, color in zip([\"Question\", \"Prompt\", \"Answer\", \"Context\"], [\"blue\", \"magenta\", \"red\", \"green\"]):\n",
        "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
        "    return text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T21:52:30.474444Z",
          "iopub.execute_input": "2024-04-05T21:52:30.474943Z",
          "iopub.status.idle": "2024-04-05T21:52:30.480137Z",
          "shell.execute_reply.started": "2024-04-05T21:52:30.474911Z",
          "shell.execute_reply": "2024-04-05T21:52:30.479119Z"
        },
        "trusted": true,
        "id": "rUxZd9wxSI-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the RAG system"
      ],
      "metadata": {
        "id": "UBP4La6HSI-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_system = RAGSystem(ai_agent)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T21:52:32.259229Z",
          "iopub.execute_input": "2024-04-05T21:52:32.260118Z",
          "iopub.status.idle": "2024-04-05T21:52:35.483929Z",
          "shell.execute_reply.started": "2024-04-05T21:52:32.260082Z",
          "shell.execute_reply": "2024-04-05T21:52:35.48293Z"
        },
        "trusted": true,
        "id": "xSW5KYH8SI-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try first with few of the questions from the data we used for the retrieval system."
      ],
      "metadata": {
        "id": "qlzE-QxhSI-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = rag_system.query(data_df.iloc[0].question)\n",
        "display(Markdown(colorize_text(answer)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T21:52:35.485642Z",
          "iopub.execute_input": "2024-04-05T21:52:35.485939Z",
          "iopub.status.idle": "2024-04-05T21:52:51.107428Z",
          "shell.execute_reply.started": "2024-04-05T21:52:35.485915Z",
          "shell.execute_reply": "2024-04-05T21:52:51.106434Z"
        },
        "trusted": true,
        "id": "AF3f7NPtSI-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = rag_system.query(data_df.iloc[3].question)\n",
        "display(Markdown(colorize_text(answer)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T21:52:51.109295Z",
          "iopub.execute_input": "2024-04-05T21:52:51.109581Z",
          "iopub.status.idle": "2024-04-05T21:53:32.336136Z",
          "shell.execute_reply.started": "2024-04-05T21:52:51.109557Z",
          "shell.execute_reply": "2024-04-05T21:53:32.335242Z"
        },
        "trusted": true,
        "id": "dkZ9t9iFSI-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = rag_system.query(\"What’s the normal distribution? Why do we care about it?\")\n",
        "display(Markdown(colorize_text(answer)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T21:53:32.337216Z",
          "iopub.execute_input": "2024-04-05T21:53:32.337526Z"
        },
        "trusted": true,
        "id": "CNap36zJSI-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try also with some \"fresh\" questions."
      ],
      "metadata": {
        "id": "yvSwNYZfSI-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = rag_system.query(\"Please explain bias and variance?\")\n",
        "display(Markdown(colorize_text(answer)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-05T20:28:37.108094Z",
          "iopub.execute_input": "2024-04-05T20:28:37.108685Z",
          "iopub.status.idle": "2024-04-05T20:29:59.558841Z",
          "shell.execute_reply.started": "2024-04-05T20:28:37.108649Z",
          "shell.execute_reply": "2024-04-05T20:29:59.557833Z"
        },
        "trusted": true,
        "id": "AOnBJhzjSI-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = rag_system.query(\"What is a Dropout?\")\n",
        "display(Markdown(colorize_text(answer)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "03sdn2eBSI-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "We tested a RAG system developed with Gemma as LLM, Langchain for data loaders utilities, and ChromaDB as database.\n",
        "The RAG system is initialized with a dataset, that is used to populate the vector database, and with an AI Agent, that will query Gemma, given the initial query and the retrieved context.\n",
        "To verify that the result is composed based on the context provided, we include as well the context in the exported result.\n"
      ],
      "metadata": {
        "id": "q3stgkq2SI-1"
      }
    }
  ]
}